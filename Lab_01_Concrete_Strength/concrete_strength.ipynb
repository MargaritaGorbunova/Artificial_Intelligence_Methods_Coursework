# %% [markdown]
# # Lab 01: Concrete Strength Prediction
# 
# ## Implementation of Single-Layer and Multi-Layer Perceptrons from Scratch
# 
# **Task:** Predict concrete compressive strength based on 8 compositional parameters using neural networks implemented from scratch.
# 
# **Dataset:** [Concrete Compressive Strength](https://www.kaggle.com/datasets/elikplim/concrete-compressive-strength-data-set) on Kaggle
# 
# **Key Points:**
# - Implementation of single-layer perceptron (Widrow-Hoff algorithm)
# - Implementation of multi-layer perceptron with backpropagation
# - Comparison of both architectures
# - Early stopping mechanism
# 
# **Results:** Multi-layer perceptron reduced MSE by 15.6% compared to single-layer.

# %% [markdown]
# ## 1. Import Libraries

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Set style for better visualizations
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# %% [markdown]
# ## 2. Load and Explore Data

# %%
# Load dataset
df = pd.read_csv("concrete_data.csv")

print("=" * 50)
print("DATASET OVERVIEW")
print("=" * 50)
print(f"Shape: {df.shape[0]} samples, {df.shape[1]} features")
print("\nFirst 5 rows:")
print(df.head())
print("\nDataset info:")
print(df.info())
print("\nStatistical summary:")
print(df.describe())
print("\nMissing values:")
print(df.isnull().sum())

# Features and target
X = df.drop("Strength", axis=1)
y = df["Strength"]

print(f"\nFeatures: {list(X.columns)}")
print(f"Target: Strength (concrete compressive strength in MPa)")

# %% [markdown]
# ## 3. Exploratory Data Analysis (EDA)

# %%
# Histograms for all variables
fig, axes = plt.subplots(3, 3, figsize=(15, 12))
fig.suptitle('Distribution of Features and Target', fontsize=16, fontweight='bold')

features = list(df.columns)
for idx, feature in enumerate(features):
    ax = axes[idx // 3, idx % 3]
    df[feature].hist(bins=30, ax=ax, edgecolor='black')
    ax.set_title(feature, fontweight='bold')
    ax.set_xlabel('Value')
    ax.set_ylabel('Frequency')

plt.tight_layout()
plt.show()

# Correlation matrix
plt.figure(figsize=(12, 8))
correlation_matrix = df.corr()
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
            mask=mask, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# Strength correlation with features
strength_corr = correlation_matrix['Strength'].sort_values(ascending=False)[1:]
print("Correlation with Strength (target):")
print(strength_corr)

# %% [markdown]
# ## 4. Data Preprocessing

# %%
# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, shuffle=True
)

print("Data split:")
print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")
print(f"Number of features: {X_train.shape[1]}")

# %% [markdown]
# ## 5. Single-Layer Perceptron Implementation

# %%
class SingleLayerPerceptron:
    """Single-layer perceptron implementing the Widrow-Hoff learning rule."""
    
    def __init__(self, learning_rate=0.01, epochs=30, tol=1e-4):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.tol = tol
        self.weights = None
        self.bias = None
        self.train_errors = []
        self.test_errors = []
        
    def initialize_weights(self, n_features):
        """Initialize weights with random values."""
        self.weights = np.random.randn(n_features)
        self.bias = np.random.randn()
        
    def predict(self, X):
        """Make predictions: y = Xw + b."""
        return np.dot(X, self.weights) + self.bias
    
    def train(self, X_train, y_train, X_test, y_test):
        """Train the perceptron using online gradient descent."""
        self.initialize_weights(X_train.shape[1])
        prev_train_error = float('inf')
        
        for epoch in range(self.epochs):
            # Online learning: update weights for each sample
            for i in range(X_train.shape[0]):
                xi = X_train[i]
                yi = y_train.iloc[i] if hasattr(y_train, 'iloc') else y_train[i]
                y_pred = self.predict(xi)
                error = yi - y_pred
                
                # Update weights using Widrow-Hoff rule
                self.weights += self.learning_rate * error * xi
                self.bias += self.learning_rate * error
            
            # Calculate MSE after each epoch
            train_pred = self.predict(X_train)
            test_pred = self.predict(X_test)
            
            train_mse = np.mean((y_train - train_pred) ** 2)
            test_mse = np.mean((y_test - test_pred) ** 2)
            
            self.train_errors.append(train_mse)
            self.test_errors.append(test_mse)
            
            # Early stopping condition
            if abs(prev_train_error - train_mse) < self.tol:
                print(f"Early stopping at epoch {epoch + 1}")
                break
                
            prev_train_error = train_mse
            
            # Progress tracking
            if (epoch + 1) % 5 == 0:
                print(f"Epoch {epoch + 1}/{self.epochs} - Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}")
        
        return self.train_errors, self.test_errors
    
    def evaluate(self, X_train, y_train, X_test, y_test):
        """Evaluate model performance."""
        train_pred = self.predict(X_train)
        test_pred = self.predict(X_test)
        
        train_mse = mean_squared_error(y_train, train_pred)
        test_mse = mean_squared_error(y_test, test_pred)
        
        print("=" * 50)
        print("SINGLE-LAYER PERCEPTRON RESULTS")
        print("=" * 50)
        print(f"Training MSE: {train_mse:.4f}")
        print(f"Test MSE: {test_mse:.4f}")
        
        return train_mse, test_mse
    
    def plot_training_history(self):
        """Plot training and test errors during training."""
        plt.figure(figsize=(10, 6))
        epochs_range = range(1, len(self.train_errors) + 1)
        
        plt.plot(epochs_range, self.train_errors, label='Training Error', linewidth=2)
        plt.plot(epochs_range, self.test_errors, label='Test Error', linewidth=2, linestyle='--')
        
        plt.title('Single-Layer Perceptron: Training Progress', fontsize=16, fontweight='bold')
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Mean Squared Error (MSE)', fontsize=12)
        plt.legend(fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

# %% [markdown]
# ## 6. Train and Evaluate Single-Layer Perceptron

# %%
# Initialize and train single-layer perceptron
slp = SingleLayerPerceptron(learning_rate=0.01, epochs=30, tol=1e-4)

print("Training Single-Layer Perceptron...")
print("-" * 40)
train_errors_slp, test_errors_slp = slp.train(X_train, y_train, X_test, y_test)
train_mse_slp, test_mse_slp = slp.evaluate(X_train, y_train, X_test, y_test)

# Plot training history
slp.plot_training_history()

# %% [markdown]
# ## 7. Multi-Layer Perceptron Implementation

# %%
class MultiLayerPerceptron:
    """Multi-layer perceptron with one hidden layer and backpropagation."""
    
    def __init__(self, input_size, hidden_size, output_size):
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # Initialize weights with Xavier/Glorot initialization
        limit_input = np.sqrt(6 / (input_size + hidden_size))
        self.weights_input_hidden = np.random.uniform(
            -limit_input, limit_input, (input_size, hidden_size)
        )
        
        limit_hidden = np.sqrt(6 / (hidden_size + output_size))
        self.weights_hidden_output = np.random.uniform(
            -limit_hidden, limit_hidden, (hidden_size, output_size)
        )
        
        # Initialize biases
        self.bias_hidden = np.zeros(hidden_size)
        self.bias_output = np.zeros(output_size)
        
        # Training history
        self.train_errors = []
        self.test_errors = []
        
    def relu(self, x):
        """ReLU activation function."""
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        """Derivative of ReLU function."""
        return (x > 0).astype(float)
    
    def forward(self, X):
        """Forward pass through the network."""
        # Hidden layer
        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden
        self.hidden_output = self.relu(self.hidden_input)
        
        # Output layer (linear activation for regression)
        self.output = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output
        
        return self.output
    
    def backward(self, X, y, output, learning_rate):
        """Backward pass (backpropagation)."""
        # Calculate errors
        error = y - output
        
        # Output layer gradients
        d_output = -error  # For MSE loss: dL/dy = -(y_true - y_pred)
        
        # Hidden layer gradients
        d_hidden = d_output.dot(self.weights_hidden_output.T)
        d_hidden *= self.relu_derivative(self.hidden_input)
        
        # Calculate gradients
        grad_weights_ho = self.hidden_output.T.dot(d_output)
        grad_bias_ho = np.sum(d_output, axis=0)
        
        grad_weights_ih = X.T.dot(d_hidden)
        grad_bias_ih = np.sum(d_hidden, axis=0)
        
        # Update weights and biases
        self.weights_hidden_output -= learning_rate * grad_weights_ho
        self.bias_output -= learning_rate * grad_bias_ho
        
        self.weights_input_hidden -= learning_rate * grad_weights_ih
        self.bias_hidden -= learning_rate * grad_bias_ih
    
    def train(self, X_train, y_train, X_test, y_test, epochs, learning_rate):
        """Train the multi-layer perceptron."""
        # Reshape y to 2D if needed
        if len(y_train.shape) == 1:
            y_train = y_train.reshape(-1, 1)
        if len(y_test.shape) == 1:
            y_test = y_test.reshape(-1, 1)
        
        for epoch in range(epochs):
            # Forward pass
            output_train = self.forward(X_train)
            
            # Backward pass
            self.backward(X_train, y_train, output_train, learning_rate)
            
            # Calculate errors
            output_test = self.forward(X_test)
            
            train_mse = np.mean((y_train - output_train) ** 2)
            test_mse = np.mean((y_test - output_test) ** 2)
            
            self.train_errors.append(train_mse)
            self.test_errors.append(test_mse)
            
            # Progress tracking
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch + 1}/{epochs} - Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}")
                
        return self.train_errors, self.test_errors
    
    def predict(self, X):
        """Make predictions."""
        return self.forward(X)
    
    def evaluate(self, X_train, y_train, X_test, y_test):
        """Evaluate model performance."""
        train_pred = self.predict(X_train)
        test_pred = self.predict(X_test)
        
        # Reshape if needed
        if len(y_train.shape) == 1:
            y_train = y_train.reshape(-1, 1)
        if len(y_test.shape) == 1:
            y_test = y_test.reshape(-1, 1)
        
        train_mse = np.mean((y_train - train_pred) ** 2)
        test_mse = np.mean((y_test - test_pred) ** 2)
        
        print("=" * 50)
        print("MULTI-LAYER PERCEPTRON RESULTS")
        print("=" * 50)
        print(f"Training MSE: {train_mse:.4f}")
        print(f"Test MSE: {test_mse:.4f}")
        
        return train_mse, test_mse
    
    def plot_training_history(self):
        """Plot training and test errors during training."""
        plt.figure(figsize=(10, 6))
        epochs_range = range(1, len(self.train_errors) + 1)
        
        plt.plot(epochs_range, self.train_errors, label='Training Error', linewidth=2)
        plt.plot(epochs_range, self.test_errors, label='Test Error', linewidth=2, linestyle='--')
        
        plt.title('Multi-Layer Perceptron: Training Progress', fontsize=16, fontweight='bold')
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Mean Squared Error (MSE)', fontsize=12)
        plt.legend(fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

# %% [markdown]
# ## 8. Train and Evaluate Multi-Layer Perceptron

# %%
# Initialize and train multi-layer perceptron
input_size = X_train.shape[1]
hidden_size = 10
output_size = 1

mlp = MultiLayerPerceptron(input_size, hidden_size, output_size)

print("Training Multi-Layer Perceptron...")
print("-" * 40)
train_errors_mlp, test_errors_mlp = mlp.train(
    X_train, y_train.values.reshape(-1, 1), 
    X_test, y_test.values.reshape(-1, 1), 
    epochs=100, 
    learning_rate=0.00001
)

train_mse_mlp, test_mse_mlp = mlp.evaluate(
    X_train, y_train.values.reshape(-1, 1), 
    X_test, y_test.values.reshape(-1, 1)
)

# Plot training history
mlp.plot_training_history()

# %% [markdown]
# ## 9. Model Comparison

# %%
# Create comparison table
results_df = pd.DataFrame({
    'Model': ['Single-Layer Perceptron', 'Multi-Layer Perceptron'],
    'Training MSE': [train_mse_slp, train_mse_mlp],
    'Test MSE': [test_mse_slp, test_mse_mlp],
    'Improvement vs SLP': ['0%', f'{((test_mse_slp - test_mse_mlp) / test_mse_slp * 100):.1f}%']
})

print("=" * 50)
print("MODEL COMPARISON")
print("=" * 50)
print(results_df.to_string(index=False))

# Visualization of comparison
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Bar chart comparison
x_pos = np.arange(len(results_df))
width = 0.35

axes[0].bar(x_pos - width/2, results_df['Training MSE'], width, label='Training MSE', alpha=0.8)
axes[0].bar(x_pos + width/2, results_df['Test MSE'], width, label='Test MSE', alpha=0.8)

axes[0].set_xlabel('Model')
axes[0].set_ylabel('Mean Squared Error (MSE)')
axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')
axes[0].set_xticks(x_pos)
axes[0].set_xticklabels(['SLP', 'MLP'])
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Error progression comparison
epochs_slp = range(1, len(train_errors_slp) + 1)
epochs_mlp = range(1, len(train_errors_mlp) + 1)

axes[1].plot(epochs_slp, test_errors_slp, label='SLP (Test)', linewidth=2)
axes[1].plot(epochs_mlp, test_errors_mlp, label='MLP (Test)', linewidth=2)
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Test MSE')
axes[1].set_title('Test Error Progression', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Improvement calculation
improvement = (test_mse_slp - test_mse_mlp) / test_mse_slp * 100
print(f"\nMulti-layer perceptron shows {improvement:.1f}% improvement in test MSE compared to single-layer perceptron.")

# %% [markdown]
# ## 10. Key Takeaways
# 
# 1. **Multi-layer perceptron outperforms single-layer**: The MLP achieved significantly lower MSE (84.53 vs 100.15), showing the importance of non-linear transformations.
# 
# 2. **Backpropagation enables deeper architectures**: The MLP successfully learned complex patterns through backpropagation.
# 
# 3. **Early stopping is effective**: The single-layer perceptron training stopped early when improvement became negligible.
# 
# 4. **Proper weight initialization matters**: Using Xavier initialization helped with training stability.
# 
# 5. **Feature scaling is crucial**: Standardizing features improved convergence for both models.

# %% [markdown]
# ## 11. How to Use This Code
# 
# To run this notebook:
# 1. Install requirements: `pip install pandas numpy matplotlib seaborn scikit-learn`
# 2. Download the dataset from Kaggle
# 3. Update the file path in the `pd.read_csv()` command
# 4. Run all cells sequentially

# %%
# Save the models (optional)
import pickle

models = {
    'single_layer': slp,
    'multi_layer': mlp,
    'scaler': scaler
}

# Uncomment to save models
# with open('concrete_strength_models.pkl', 'wb') as f:
#     pickle.dump(models, f)

print("Notebook execution completed successfully!")
